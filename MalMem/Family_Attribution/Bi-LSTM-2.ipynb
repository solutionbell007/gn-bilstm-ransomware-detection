{"cells":[{"cell_type":"code","execution_count":1,"id":"NOfwMyCAPCji","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20457,"status":"ok","timestamp":1700144660897,"user":{"displayName":"Amjad Hussain","userId":"16170331249701690369"},"user_tz":-300},"id":"NOfwMyCAPCji","outputId":"eab72d08-3e24-4312-bd1f-d8cd9f361db2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"id":"2e41a616","metadata":{"executionInfo":{"elapsed":14894,"status":"ok","timestamp":1700144678469,"user":{"displayName":"Amjad Hussain","userId":"16170331249701690369"},"user_tz":-300},"id":"2e41a616"},"outputs":[],"source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler\n","import time\n","import seaborn as sns\n","import plotly.express as px"]},{"cell_type":"markdown","id":"ca5cce15","metadata":{"id":"ca5cce15"},"source":["<h3>Preprocessing (Loading, removing and showing graphs)</h3>"]},{"cell_type":"code","execution_count":3,"id":"081ad968","metadata":{"executionInfo":{"elapsed":2236,"status":"ok","timestamp":1700144682078,"user":{"displayName":"Amjad Hussain","userId":"16170331249701690369"},"user_tz":-300},"id":"081ad968"},"outputs":[],"source":["# Load your CSV dataset (replace 'your_dataset.csv' with your actual dataset)\n","data = pd.read_csv('/content/drive/MyDrive/ML/Thesis/MalMem/Family_Attribution/family3.csv')\n","from sklearn.preprocessing import LabelEncoder\n","lab=LabelEncoder()\n","data['Class']=lab.fit_transform(data['Class'])\n","data['Category']=lab.fit_transform(data['Category'])\n","data['Family']=lab.fit_transform(data['Family'])\n","#data = data.drop(['Class'], axis=1)\n","#data = data.drop(['Category'], axis=1)\n","#data = data.drop(['Family'], axis=1)\n","#columns with 0 values\n","data = data.drop(['pslist.nprocs64bit'], axis=1)\n","data = data.drop(['handles.nport'], axis=1)\n","data = data.drop(['psxview.not_in_eprocess_pool'], axis=1)\n","data = data.drop(['psxview.not_in_eprocess_pool_false_avg'], axis=1)\n","data = data.drop(['svcscan.interactive_process_services'], axis=1)\n","data = data.drop(['callbacks.nanonymous'], axis=1)\n","data = data.drop(['modules.nmodules'], axis=1)\n","data = data.drop(['callbacks.ngeneric'], axis=1)\n","data = data.drop(['svcscan.fs_drivers'], axis=1)\n","\n","#data = data.drop(['handles.nhandles', 'dlllist.ndlls', 'dlllist.avg_dlls_per_proc', 'svcscan.process_services'], axis=1)"]},{"cell_type":"code","execution_count":4,"id":"m-eE5-NgFhry","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1700144683914,"user":{"displayName":"Amjad Hussain","userId":"16170331249701690369"},"user_tz":-300},"id":"m-eE5-NgFhry"},"outputs":[],"source":["data = data.drop_duplicates()"]},{"cell_type":"code","execution_count":5,"id":"fpbyxaT0Sx_a","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1700144685692,"user":{"displayName":"Amjad Hussain","userId":"16170331249701690369"},"user_tz":-300},"id":"fpbyxaT0Sx_a"},"outputs":[],"source":["import pandas as pd\n","from scipy import stats\n","\n","# Define a threshold for z-score, typically 2 or 3\n","z_score_threshold = 3\n","\n","# Function to remove outliers based on z-score for all columns\n","def remove_outliers_zscore_all_columns(data, threshold):\n","    z_scores = stats.zscore(data)\n","    abs_z_scores = abs(z_scores)\n","    outliers = (abs_z_scores > threshold).all(axis=1)\n","    df_no_outliers = data[~outliers]\n","    return df_no_outliers\n","\n","# Apply z-score outlier removal for all columns\n","df_no_outliers = remove_outliers_zscore_all_columns(data, z_score_threshold)\n","data = df_no_outliers"]},{"cell_type":"code","execution_count":6,"id":"KaGZuAvLFmAs","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2584,"status":"ok","timestamp":1700144689977,"user":{"displayName":"Amjad Hussain","userId":"16170331249701690369"},"user_tz":-300},"id":"KaGZuAvLFmAs","outputId":"20241e5f-4999-4ef8-c13f-cd0d4dcb9064"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply\n","  x = um.multiply(x, x, out=x)\n","/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:247: RuntimeWarning: overflow encountered in reduce\n","  ret = umr_sum(x, axis, dtype, out, keepdims=keepdims, where=where)\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from scipy.stats import skew\n","from sklearn.preprocessing import PowerTransformer\n","\n","# Assuming your dataset is stored in a pandas DataFrame called 'df'\n","\n","# Identify skewed features\n","skewness = data.apply(lambda x: skew(x))\n","skew_features = skewness[abs(skewness) > 0.5].index\n","\n","# Apply log transformation to skewed features\n","#data[skew_features] = np.log1p(data[skew_features])\n","\n","# Alternatively, you can use PowerTransformer for a more general approach\n","scaler = PowerTransformer(method='yeo-johnson', standardize=True)\n","data[skew_features] = scaler.fit_transform(data[skew_features].values)\n","\n","# Now, your dataset has reduced skewness"]},{"cell_type":"markdown","id":"9133f104","metadata":{"id":"9133f104"},"source":["<h3>Normalization</h3>"]},{"cell_type":"code","execution_count":7,"id":"deb04648","metadata":{"executionInfo":{"elapsed":427,"status":"ok","timestamp":1700144691959,"user":{"displayName":"Amjad Hussain","userId":"16170331249701690369"},"user_tz":-300},"id":"deb04648"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","# Create a Min-Max scaler instance\n","scaler = StandardScaler()\n","# Select the columns you want to scale (exclude the target variable if needed)\n","columns_to_scale = data.columns  # You can select specific columns here\n","\n","# Fit the scaler on the selected columns and transform the data\n","data[columns_to_scale] = scaler.fit_transform(data[columns_to_scale])"]},{"cell_type":"markdown","id":"147dfc4f","metadata":{"id":"147dfc4f"},"source":["<h3>Spliting for Training and Testing</h3>"]},{"cell_type":"code","execution_count":8,"id":"3f4876df","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":377,"status":"ok","timestamp":1700144694487,"user":{"displayName":"Amjad Hussain","userId":"16170331249701690369"},"user_tz":-300},"id":"3f4876df","outputId":"69306452-cf1d-41b7-fc69-64e099b8e9ed"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(30816, 48)"]},"metadata":{},"execution_count":8}],"source":["# Separate features and labels\n","X = data.iloc[:, :-1].values  # Features\n","y = data.iloc[:, -1].values   # Class labels\n","X.shape"]},{"cell_type":"code","execution_count":9,"id":"fscaS_XK26TA","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1021,"status":"ok","timestamp":1700144697141,"user":{"displayName":"Amjad Hussain","userId":"16170331249701690369"},"user_tz":-300},"id":"fscaS_XK26TA","outputId":"31b5ce53-689d-4d3e-e688-71670ab371b1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(38544, 48)"]},"metadata":{},"execution_count":9}],"source":["from sklearn.preprocessing import LabelEncoder\n","from imblearn.over_sampling import SMOTE\n","le = LabelEncoder()\n","y_encoded = le.fit_transform(y)  # Encode the target labels\n","smote = SMOTE(random_state=42)\n","\n","X_smote, y_smote = smote.fit_resample(X, y_encoded)\n","X_smote.shape"]},{"cell_type":"markdown","id":"31f0bbbe","metadata":{"id":"31f0bbbe"},"source":["<h3>Bi-LSTM with 20 Epochs Implementation</h3>"]},{"cell_type":"code","execution_count":null,"id":"fd02bfe2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fd02bfe2","outputId":"d2af0f5c-48b6-4368-f706-dd36bab4420d"},"outputs":[{"output_type":"stream","name":"stderr","text":["Fold 1/5:  35%|███▌      | 7/20 [08:56<16:38, 76.80s/it]"]}],"source":["## Define your dataset and labels (X and y) here\n","#X = np.random.randn(58596, 46)  # Replace with your actual dataset\n","#y = np.random.randint(0, 2, size=(58596,))  # Replace with your actual labels\n","# Convert data to PyTorch tensors\n","X = X_smote\n","y = y_smote\n","# Define the number of folds for cross-validation\n","num_splits = 5  # You can adjust the number of folds as needed\n","\n","# Initialize lists to store accuracy scores for each fold\n","fold_accuracies = []\n","# Initialize lists to store time taken for training and testing\n","train_times = []\n","test_times = []\n","\n","# Initialize lists to store true labels and predicted labels for confusion matrix\n","all_true_labels = []\n","all_predicted_labels = []\n","\n","# Initialize the cross-validator\n","kf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n","\n","# Define a Bi-LSTM model\n","class BiLSTMGRU(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n","        super(BiLSTMGRU, self).__init__()\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n","        self.gru = nn.GRU(2 * hidden_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n","        self.fc = nn.Linear(2 * hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        lstm_out, _ = self.lstm(x)\n","        gru_out, _ = self.gru(lstm_out)\n","        output = self.fc(gru_out)\n","        return output\n","# Specify the model hyperparameters\n","input_size = 48   # Number of input features\n","hidden_size = 128  # LSTM hidden layer size\n","num_layers = 2  # Number of LSTM layers\n","num_classes = 16  # Number of classes\n","\n","# Create an instance of the Bi-LSTM model\n","model = BiLSTMGRU(input_size, hidden_size, num_layers, num_classes)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # You can adjust the learning rate\n","\n","# Training loop\n","num_epochs = 20  # Specify the number of training epochs\n","train_losses = []\n","test_losses = []\n","train_accuracies = []\n","test_accuracies = []\n","\n","# Initialize a variable to store the total training loss\n","total_training_loss = 0.0\n","total_batches = 0\n","\n","for train_index, test_index in kf.split(X, y):\n","    start_time = time.time()\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","\n","    # Convert data to PyTorch tensors\n","    X_train = torch.tensor(X_train, dtype=torch.float32)\n","    X_test = torch.tensor(X_test, dtype=torch.float32)\n","    y_train = torch.tensor(y_train, dtype=torch.long)\n","    y_test = torch.tensor(y_test, dtype=torch.long)\n","\n","    # Create DataLoader for training and testing\n","    train_dataset = TensorDataset(X_train, y_train)\n","    test_dataset = TensorDataset(X_test, y_test)\n","\n","    batch_size = 32\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    for epoch in tqdm(range(num_epochs), desc=f'Fold {len(fold_accuracies) + 1}/{num_splits}'):\n","        model.train()\n","        correct_train = 0\n","        total_train = 0\n","        running_train_loss = 0.0\n","\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_train_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total_train += labels.size(0)\n","            correct_train += (predicted == labels).sum().item()\n","            total_batches += 1\n","\n","        # Accumulate the training loss for this batch\n","        total_training_loss += running_train_loss\n","\n","        train_accuracy = correct_train / total_train\n","        train_losses.append(running_train_loss / len(train_loader))\n","        train_accuracies.append(train_accuracy)\n","\n","        # Calculate time taken for training this epoch\n","        train_time_epoch = time.time() - start_time\n","        train_times.append(train_time_epoch)\n","\n","        # Evaluation on the test set\n","        model.eval()\n","        correct_test = 0\n","        total_test = 0\n","        running_test_loss = 0.0\n","        start_time = time.time()  # Start timing the testing phase\n","\n","        for inputs, labels in test_loader:\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            running_test_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            total_test += labels.size(0)\n","            correct_test += (predicted == labels).sum().item()\n","\n","            # Store true and predicted labels for the confusion matrix\n","            all_true_labels.extend(labels.cpu().numpy())\n","            all_predicted_labels.extend(predicted.cpu().numpy())\n","\n","        # Calculate time taken for testing this epoch\n","        test_time_epoch = time.time() - start_time\n","        test_times.append(test_time_epoch)\n","\n","        test_accuracy = correct_test / total_test\n","        test_losses.append(running_test_loss / len(test_loader))\n","        test_accuracies.append(test_accuracy)\n","\n","    # Store accuracy for this fold\n","    fold_accuracies.append(test_accuracy)\n","\n","# Print and visualize results (training and test accuracy, loss)\n","print(f\"Mean Accuracy Across Folds: {np.mean(fold_accuracies) * 100:.2f}%\")\n","# Calculate the mean training loss\n","mean_training_loss = total_training_loss / total_batches\n","\n","# Print the mean training loss\n","print(f'Mean Training Loss: {mean_training_loss:.4f}')\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(range(len(train_accuracies)), train_accuracies, label=\"Train\")\n","plt.plot(range(len(test_accuracies)), test_accuracies, label=\"Test\")\n","plt.title(\"Accuracy vs. Epoch\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.savefig('/content/drive/MyDrive/ML/Thesis/MalMem/Family_Attribution/graphs/bi-lstm-gru/20_ep_accuracy.pdf', format='pdf')\n","plt.show()\n","\n","plt.close()\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(range(len(train_losses)), train_losses, label=\"Train\")\n","plt.plot(range(len(test_losses)), test_losses, label=\"Test\")\n","plt.title(\"Loss vs. Epoch\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.savefig('/content/drive/MyDrive/ML/Thesis/MalMem/Family_Attribution/graphs/bi-lstm-gru/20_ep_loss.pdf', format='pdf')\n","plt.show()\n","plt.close()\n","\n","# Calculate and display the confusion matrix for the last fold\n","model.eval()\n","conf_matrix = confusion_matrix(all_true_labels, all_predicted_labels)\n","\n","# Plot the confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix')\n","plt.savefig('/content/drive/MyDrive/ML/Thesis/MalMem/Family_Attribution/graphs/bi-lstm-gru/20_ep_confusion.pdf', format='pdf')\n","plt.show()\n","plt.close()\n","\n","# Calculate and print the mean training and testing times\n","mean_train_time = np.mean(train_times)\n","mean_test_time = np.mean(test_times)\n","print(f'Mean Training Time (per epoch): {mean_train_time:.2f} seconds')\n","print(f'Mean Testing Time (per epoch): {mean_test_time:.2f} seconds')"]},{"cell_type":"markdown","id":"e6df123d","metadata":{"id":"e6df123d"},"source":["<h3>Bi-LSTM with 30 Epochs Implementation</h3>"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":5}